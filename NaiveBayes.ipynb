{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3926272674078408\n",
      "Predicted classes: ['positive' 'very negative']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SST dataset\n",
    "dataset = load_dataset(\"sst\", \"default\")\n",
    "\n",
    "# Access the training split of the dataset\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Extract features (X) and labels (y) from the training split\n",
    "X = train_dataset[\"sentence\"]\n",
    "sentiment_scores = np.array(train_dataset[\"label\"])\n",
    "\n",
    "# Convert sentiment scores to discrete class labels using the mapping function\n",
    "def mapping(a):\n",
    "    if a <= 0.2:\n",
    "        return \"very negative\"\n",
    "    elif a <= 0.4:\n",
    "        return 'negative'\n",
    "    elif a <= 0.6:\n",
    "        return 'neutral'\n",
    "    elif a <= 0.8:\n",
    "        return 'positive'\n",
    "    elif a<=1 and a>0.8:\n",
    "        return 'very positive'\n",
    "\n",
    "y = np.array([mapping(score) for score in sentiment_scores])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with CountVectorizer and Multinomial Naive Bayes\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Predict the class for new instances\n",
    "new_sentences = [\"I love this movie!\", \"This movie is terrible.\"]\n",
    "predicted_classes = model.predict(new_sentences)\n",
    "print(\"Predicted classes:\", predicted_classes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T19:15:58.057777Z",
     "start_time": "2024-03-15T19:15:50.966809Z"
    }
   },
   "id": "1ef610a832643276",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in training dataset: 8544\n",
      "Number of labels in training dataset: 8544\n",
      "Predicted: 1, True: 1.0, Document: It 's the best film of the year so far , the benchmark against which all other Best Picture contenders should be measured .\n",
      "Predicted: 1, True: 1.0, Document: Aside from being the funniest movie of the year , Simone , Andrew Niccol 's brilliant anti-Hollywood satire , has a wickedly eccentric enchantment to it .\n",
      "Predicted: 1, True: 1.0, Document: A stunning piece of visual poetry that will , hopefully , be remembered as one of the most important stories to be told in Australia 's film history .\n",
      "Predicted: 1, True: 1.0, Document: Invincible is a wonderful movie .\n",
      "Accuracy: 0.0018099547511312218\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SST dataset\n",
    "dataset = load_dataset(\"sst\", \"default\")\n",
    "\n",
    "# Access the training split of the dataset\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Extract features (xs) and labels (ys) from the training split\n",
    "sentence = train_dataset[\"sentence\"]\n",
    "labels = train_dataset[\"label\"]\n",
    "\n",
    "# Convert labels to numpy array if needed\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"Number of sentences in training dataset:\", len(sentence))\n",
    "print(\"Number of labels in training dataset:\", len(labels))\n",
    "\n",
    "\n",
    "def maping(a):\n",
    "    if a <= 0.2:\n",
    "        return \"very negative\"\n",
    "    elif a <= 0.4:\n",
    "        return 'negative'\n",
    "    elif a <= 0.6:\n",
    "        return 'neutral'\n",
    "    elif a <= 0.8:\n",
    "        return 'positive'\n",
    "    elif a <= 1 and a > 0.8:\n",
    "        return 'very positive'\n",
    "\n",
    "# Apply mapping function to labels\n",
    "classes = np.vectorize(maping)(labels)\n",
    "\n",
    "\n",
    "def train_naive_bayes(D, C):\n",
    "    # Calculate the number of documents in each class.\n",
    "    class_counts = {c: 0 for c in C}\n",
    "    for doc in D:\n",
    "        class_counts[doc['class']] += 1\n",
    "\n",
    "    # Calculate the log-prior probabilities of each class.\n",
    "    log_prior = {c: np.log(class_counts[c] / len(D)) for c in C}\n",
    "\n",
    "    # Create a dictionary to store the word counts for each class.\n",
    "    word_counts_by_class = {c: {} for c in C}\n",
    "\n",
    "    # Create a vocabulary of all unique words in the training data.\n",
    "    vocabulary = set()\n",
    "    for doc in D:\n",
    "        for word in doc['text'].split():\n",
    "            vocabulary.add(word)\n",
    "            word_counts_by_class[doc['class']][word] = word_counts_by_class[doc['class']].get(word, 0) + 1  # .get(word,0)\n",
    "\n",
    "    # Calculate the log-likelihood probabilities of each word given each class.\n",
    "    log_likelihood = {}\n",
    "    for word in vocabulary:\n",
    "        log_likelihood[word] = {c: np.log((word_counts_by_class[c].get(word, 0) + 1) /\n",
    "                                          (sum(word_counts_by_class[c].values()) + len(vocabulary)))\n",
    "                                 for c in C}\n",
    "\n",
    "    return log_prior, log_likelihood, vocabulary\n",
    "\n",
    "\n",
    "# Prepare documents\n",
    "documents = [{'text': s, 'class': c} for s, c in zip(sentence, classes)]\n",
    "\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "log_prior, log_likelihood, vocabulary = train_naive_bayes(documents, set(classes))\n",
    "\n",
    "# Convert vocabulary set to a list\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "\n",
    "def TestNaiveBayes(TestDoc, logPrior, log_likelihood, vocabulary):\n",
    "    Sum = np.zeros(len(logPrior))  # Get the number of classes from logPrior\n",
    "    best_c = None\n",
    "    max_Sum = -np.inf\n",
    "\n",
    "    for c, class_label in enumerate(logPrior):\n",
    "        Sum[c] = logPrior[class_label]\n",
    "        for word in TestDoc:\n",
    "            if word in vocabulary and c in log_likelihood.get(word, {}):  # Check if word exists for class c\n",
    "                Sum[c] += log_likelihood[word][c]\n",
    "        if Sum[c] > max_Sum:\n",
    "            max_Sum = Sum[c]\n",
    "            best_c = c\n",
    "    return best_c\n",
    "\n",
    "\n",
    "# Access the testing split of the dataset\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# Extract features (xs) and labels (ys) from the testing split\n",
    "test_sentences = test_dataset[\"sentence\"]\n",
    "test_labels = test_dataset[\"label\"]\n",
    "\n",
    "# Initialize variables for accuracy calculation\n",
    "total_correct = 0\n",
    "total_samples = len(test_sentences)\n",
    "\n",
    "# Iterate over the test dataset and make predictions\n",
    "for test_doc, true_class in zip(test_sentences, test_labels):\n",
    "    predicted_class = TestNaiveBayes(test_doc.split(), log_prior, log_likelihood, vocabulary)\n",
    "    if predicted_class == true_class:\n",
    "        total_correct += 1\n",
    "        print(f\"Predicted: {predicted_class}, True: {true_class}, Document: {test_doc}\")\n",
    "    # else:\n",
    "    #     print(f\"Predicted: {predicted_class}, True: {true_class}, Document: {test_doc}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = total_correct / total_samples\n",
    "print(\"Accuracy:\", accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T21:29:32.335320Z",
     "start_time": "2024-03-15T21:28:55.104070Z"
    }
   },
   "id": "6d0e7b6daed05cea",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6afc484a9eef737c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
