{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:15:22.217785700Z",
     "start_time": "2024-03-15T23:14:52.782904600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\",\n        0.6944400072097778,\n        \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\",\n        '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'],\n       [\"The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\",\n        0.833329975605011,\n        \"The|gorgeously|elaborate|continuation|of|``|The|Lord|of|the|Rings|''|trilogy|is|so|huge|that|a|column|of|words|can|not|adequately|describe|co-writer\\\\/director|Peter|Jackson|'s|expanded|vision|of|J.R.R.|Tolkien|'s|Middle-earth|.\",\n        '71|70|69|69|67|67|66|64|63|62|62|61|61|58|57|57|56|53|53|52|52|49|49|50|48|45|44|43|43|42|42|41|39|38|38|40|60|39|40|41|47|46|44|45|46|47|48|51|50|51|55|54|54|55|56|59|58|59|60|73|65|63|64|65|66|68|68|72|70|71|72|73|0'],\n       ['Singer\\\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .',\n        0.625,\n        'Singer\\\\/composer|Bryan|Adams|contributes|a|slew|of|songs|--|a|few|potential|hits|,|a|few|more|simply|intrusive|to|the|story|--|but|the|whole|package|certainly|captures|the|intended|,|er|,|spirit|of|the|piece|.',\n        '72|71|71|70|68|68|67|67|66|63|62|62|60|60|58|58|56|55|55|54|53|53|65|75|51|50|50|49|48|46|44|43|42|42|45|41|40|40|77|41|47|43|44|45|46|47|48|49|52|51|52|76|54|57|56|57|59|59|61|61|64|63|64|65|66|74|69|69|70|73|72|73|74|75|76|77|0'],\n       ...,\n       [\"We 've seen the hippie-turned-yuppie plot before , but there 's an enthusiastic charm in Fire that makes the formula fresh again .\",\n        0.75,\n        \"We|'ve|seen|the|hippie-turned-yuppie|plot|before|,|but|there|'s|an|enthusiastic|charm|in|Fire|that|makes|the|formula|fresh|again|.\",\n        '41|40|38|37|36|36|39|42|43|35|34|31|30|30|29|29|28|27|25|25|24|24|45|26|26|27|28|33|32|31|32|33|34|35|44|37|38|39|40|41|42|43|44|45|0'],\n       [\"Her fans walked out muttering words like `` horrible '' and `` terrible , '' but had so much fun dissing the film that they did n't mind the ticket cost .\",\n        0.13888999819755554,\n        \"Her|fans|walked|out|muttering|words|like|``|horrible|''|and|``|terrible|,|''|but|had|so|much|fun|dissing|the|film|that|they|did|n't|mind|the|ticket|cost|.\",\n        '62|62|55|55|53|53|52|51|50|49|48|47|47|57|58|59|46|43|43|44|41|40|40|39|38|36|36|35|34|33|33|61|34|35|37|37|38|39|42|41|42|45|44|45|46|60|48|49|50|51|52|54|54|56|56|57|58|59|60|61|63|63|0'],\n       ['In this case zero .', 0.3472200036048889, 'In|this|case|zero|.',\n        '8|8|7|6|6|7|9|9|0']], dtype=object)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"sst\", \"default\")\n",
    "train = dataset['train'].to_pandas().to_numpy()\n",
    "test=dataset['test'].to_pandas().to_numpy()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{('of', 'the'): 821,\n (',', 'but'): 702,\n (',', 'and'): 666,\n ('in', 'the'): 414,\n ('It', \"'s\"): 398,\n ('it', \"'s\"): 388,\n ('is', 'a'): 371,\n ('the', 'film'): 340,\n ('of', 'a'): 332,\n (',', 'the'): 320,\n ('and', 'the'): 269,\n ('to', 'the'): 260,\n (',', 'it'): 248,\n ('to', 'be'): 243,\n ('the', 'movie'): 198,\n (\"'s\", 'a'): 197,\n ('with', 'a'): 191,\n ('for', 'the'): 190,\n ('on', 'the'): 185,\n ('but', 'it'): 185,\n ('in', 'a'): 180,\n ('as', 'a'): 178,\n ('and', 'a'): 173,\n ('it', 'is'): 169,\n ('does', \"n't\"): 165,\n ('The', 'film'): 146,\n ('a', 'movie'): 139,\n ('that', 'it'): 134,\n ('one', 'of'): 133,\n ('with', 'the'): 132,\n ('film', 'is'): 129,\n ('is', \"n't\"): 128,\n (',', 'a'): 127,\n ('that', \"'s\"): 125,\n ('it', '.'): 124,\n ('film', '.'): 121,\n ('of', 'its'): 118,\n ('for', 'a'): 117,\n ('like', 'a'): 115,\n ('movie', 'is'): 109,\n ('at', 'the'): 108,\n ('movie', '.'): 107,\n ('This', 'is'): 106,\n ('film', 'that'): 106,\n (',', 'this'): 105,\n ('from', 'the'): 105,\n ('as', 'the'): 105,\n ('the', 'most'): 102,\n ('is', 'the'): 102,\n ('all', 'the'): 100,\n ('The', 'movie'): 100,\n ('a', 'film'): 99,\n ('film', \"'s\"): 98,\n ('movie', 'that'): 98,\n (',', 'with'): 97,\n ('movie', ','): 95,\n ('that', 'is'): 94,\n ('about', 'the'): 94,\n ('film', ','): 94,\n (',', 'you'): 91,\n ('to', 'a'): 91,\n ('but', 'the'): 89,\n ('kind', 'of'): 89,\n (\"'s\", 'not'): 88,\n ('do', \"n't\"): 87,\n ('by', 'the'): 86,\n ('into', 'the'): 85,\n (',', 'is'): 84,\n ('There', \"'s\"): 83,\n ('has', 'a'): 83,\n ('have', 'been'): 83,\n ('more', 'than'): 82,\n ('in', 'its'): 82,\n ('you', \"'re\"): 81,\n ('that', 'the'): 81,\n ('out', 'of'): 80,\n ('.', \"'\"): 79,\n ('ca', \"n't\"): 79,\n ('in', 'this'): 77,\n ('this', 'movie'): 76,\n ('to', 'make'): 75,\n ('is', 'that'): 75,\n (',', 'as'): 74,\n ('as', 'it'): 72,\n ('sense', 'of'): 72,\n ('the', 'story'): 71,\n ('If', 'you'): 71,\n ('a', 'good'): 70,\n ('be', 'a'): 69,\n ('its', 'own'): 69,\n (',', 'which'): 69,\n ('this', 'is'): 68,\n ('there', \"'s\"): 68,\n ('is', 'an'): 66,\n ('the', 'best'): 65,\n ('--', 'and'): 65,\n ('enough', 'to'): 64,\n ('from', 'a'): 64,\n (',', 'I'): 64,\n ('into', 'a'): 63,\n ('a', 'lot'): 62,\n ('time', '.'): 61,\n ('is', 'not'): 61,\n ('the', 'characters'): 61,\n ('of', 'an'): 61,\n ('this', 'film'): 61,\n ('is', 'so'): 60,\n ('...', 'a'): 60,\n ('to', 'see'): 60,\n ('of', 'this'): 60,\n ('than', 'a'): 59,\n (',', 'in'): 59,\n ('the', 'end'): 57,\n ('on', 'a'): 57,\n ('look', 'at'): 57,\n ('the', 'same'): 57,\n ('of', 'his'): 56,\n ('funny', ','): 56,\n ('the', 'first'): 56,\n ('comedy', '.'): 56,\n ('a', 'few'): 55,\n ('One', 'of'): 55,\n ('and', 'it'): 55,\n ('to', 'watch'): 54,\n ('and', ','): 53,\n ('by', 'a'): 53,\n ('it', 'does'): 53,\n ('of', 'those'): 51,\n ('than', 'the'): 51,\n ('may', 'be'): 51,\n ('hard', 'to'): 50,\n ('movie', \"'s\"): 49,\n ('the', 'way'): 49,\n ('is', 'one'): 49,\n ('a', 'little'): 49,\n ('a', 'great'): 48,\n ('if', 'you'): 48,\n ('story', '.'): 47,\n ('and', 'his'): 47,\n ('at', 'least'): 46,\n ('It', 'is'): 46,\n (',', 'even'): 46,\n (',', 'that'): 45,\n ('work', '.'): 45,\n ('so', 'much'): 45,\n (',', 'who'): 45,\n ('want', 'to'): 45,\n ('such', 'a'): 45,\n ('would', 'be'): 45,\n ('most', 'of'): 45,\n ('in', 'which'): 45,\n ('it', ','): 44,\n ('you', \"'ll\"): 44,\n ('will', 'be'): 44,\n ('to', 'have'): 44,\n ('not', 'a'): 44,\n ('have', 'a'): 44,\n ('if', 'it'): 44,\n (',', 'if'): 44,\n ('wo', \"n't\"): 43,\n (',', 'or'): 43,\n ('as', 'if'): 43,\n ('about', 'a'): 42,\n ('and', 'an'): 42,\n ('you', 'can'): 41,\n ('through', 'the'): 41,\n ('and', 'its'): 41,\n ('that', 'you'): 41,\n ('the', 'year'): 41,\n ('romantic', 'comedy'): 41,\n ('the', 'audience'): 41,\n ('can', 'be'): 40,\n ('them', '.'): 40,\n ('could', 'have'): 40,\n ('manages', 'to'): 40,\n ('portrait', 'of'): 40,\n ('for', 'its'): 40,\n ('the', 'kind'): 40,\n (\"'s\", ')'): 40,\n ('I', \"'m\"): 39,\n (\"'s\", 'the'): 39,\n ('one', '.'): 39,\n ('would', 'have'): 38,\n ('with', 'an'): 38,\n ('is', ','): 38,\n (\"''\", 'is'): 38,\n ('trying', 'to'): 38,\n ('that', ','): 38,\n (',', 'not'): 38,\n ('lot', 'of'): 38,\n (',', 'so'): 37,\n ('up', 'to'): 37,\n ('he', \"'s\"): 36,\n ('story', ','): 36,\n ('you', \"'ve\"): 36,\n ('some', 'of'): 36,\n (\"'s\", 'no'): 36,\n ('life', '.'): 36,\n ('did', \"n't\"): 36,\n ('of', 'it'): 35,\n ('characters', '.'): 35,\n ('seems', 'to'): 35,\n ('funny', '.'): 35,\n ('the', 'plot'): 35,\n ('should', 'be'): 35,\n ('there', 'is'): 34,\n ('in', 'his'): 34,\n ('with', 'its'): 33,\n ('characters', ','): 33,\n ('sort', 'of'): 33,\n ('.', \"''\"): 33,\n ('a', 'bit'): 33,\n (\"n't\", 'have'): 33,\n ('to', 'its'): 33,\n ('the', 'worst'): 33,\n ('rather', 'than'): 33,\n ('a', 'bad'): 33,\n ('has', 'the'): 33,\n ('as', 'an'): 32,\n ('comedy', 'that'): 32,\n ('the', 'original'): 32,\n ('piece', 'of'): 32,\n ('to', 'keep'): 32,\n (\"'s\", 'film'): 32,\n ('is', 'more'): 32,\n ('the', 'screen'): 32,\n (',', 'The'): 32,\n ('has', 'been'): 31,\n (',', 'though'): 31,\n ('a', 'story'): 31,\n ('it', 'was'): 31,\n ('tale', 'of'): 31,\n ('to', 'do'): 31,\n ('a', 'very'): 31,\n ('the', 'world'): 31,\n ('too', 'much'): 31,\n ('this', 'one'): 30,\n ('The', 'story'): 30,\n (')', ','): 30,\n (\"'s\", 'just'): 30,\n ('to', 'find'): 30,\n (',', 'like'): 30,\n ('film', 'has'): 30,\n ('the', 'right'): 29,\n ('film', 'with'): 29,\n ('be', '.'): 29,\n (',', 'there'): 29,\n ('as', 'well'): 29,\n ('to', 'get'): 29,\n ('and', 'more'): 29,\n ('There', 'are'): 29,\n ('much', 'of'): 29,\n ('that', 'does'): 29,\n ('full', 'of'): 29,\n ('on', 'its'): 29,\n ('end', ','): 29,\n ('be', 'the'): 28,\n ('not', 'be'): 28,\n ('it', 'all'): 28,\n ('story', 'is'): 28,\n ('is', '.'): 28,\n ('...', 'but'): 28,\n (',', 'one'): 28,\n ('at', 'a'): 28,\n ('not', 'to'): 28,\n ('drama', '.'): 28,\n ('and', 'not'): 28,\n ('to', 'say'): 28,\n ('over', 'the'): 28,\n ('...', 'the'): 28,\n ('may', 'not'): 28,\n ('lack', 'of'): 28,\n (\"'s\", 'hard'): 28,\n ('is', 'just'): 28,\n ('a', 'more'): 28,\n ('--', 'but'): 27,\n (',', 'then'): 27,\n (',', \"'\"): 27,\n ('subject', 'matter'): 27,\n (',', 'to'): 27,\n ('like', 'the'): 27,\n ('(', 'and'): 27,\n ('version', 'of'): 27,\n ('A', 'movie'): 27,\n ('all', 'of'): 27,\n ('exercise', 'in'): 27,\n ('way', '.'): 27,\n ('screen', '.'): 27,\n ('movie', 'with'): 27,\n ('it', 'would'): 27,\n ('which', 'is'): 27,\n ('There', 'is'): 27,\n ('documentary', '.'): 27,\n ('that', 'will'): 27,\n ('because', 'it'): 27,\n ('better', 'than'): 27,\n ('too', 'many'): 27,\n ('and', 'that'): 26,\n (\"'s\", 'an'): 26,\n ('the', 'theater'): 26,\n ('is', 'as'): 26,\n ('in', 'an'): 26,\n ('that', 'makes'): 26,\n ('comedy', ','): 26,\n ('film', 'about'): 26,\n ('is', 'to'): 26,\n (',', 'an'): 26,\n ('if', 'the'): 26,\n ('but', 'not'): 26,\n (',', 'for'): 26,\n ('A', 'film'): 26,\n ('than', 'it'): 26,\n ('is', 'in'): 26,\n ('--', 'the'): 26,\n (',', 'we'): 26,\n ('that', 'he'): 25,\n ('going', 'to'): 25,\n ('--', 'a'): 25,\n ('by', 'its'): 25,\n ('fun', '.'): 25,\n ('year', '.'): 25,\n ('of', 'what'): 25,\n ('the', 'big'): 25,\n ('story', 'and'): 25,\n ('as', 'you'): 25,\n ('audience', '.'): 25,\n ('...', 'is'): 25,\n ('moments', 'of'): 25,\n ('In', 'the'): 25,\n ('who', 'are'): 25,\n ('all', 'its'): 25,\n ('of', 'all'): 25,\n ('characters', 'and'): 25,\n ('is', 'too'): 25,\n ('to', 'give'): 25,\n ('I', 'have'): 25,\n ('characters', 'are'): 25,\n (',', 'at'): 25,\n ('it', 'has'): 25,\n ('up', '.'): 25,\n ('here', '.'): 24,\n ('and', 'even'): 24,\n ('of', '`'): 24,\n ('it', 'a'): 24,\n ('each', 'other'): 24,\n ('of', 'their'): 24,\n ('his', 'own'): 24,\n (',', 'he'): 24,\n ('time', ','): 24,\n ('you', \"'d\"): 24,\n ('funny', 'and'): 24,\n ('but', 'there'): 24,\n ('what', 'is'): 24,\n ('might', 'have'): 24,\n (',', 'its'): 24,\n ('fails', 'to'): 24,\n (',', 'while'): 24,\n ('I', \"'ve\"): 23,\n ('when', 'it'): 23,\n ('work', 'of'): 23,\n ('the', 'director'): 23,\n ('drama', 'that'): 23,\n ('of', 'these'): 23,\n ('have', 'to'): 23,\n ('humor', '.'): 23,\n ('off', 'the'): 23,\n ('so', 'many'): 23,\n ('way', 'to'): 23,\n ('able', 'to'): 23,\n ('film', 'of'): 23,\n ('up', 'in'): 23,\n ('at', 'times'): 23,\n (',', 'no'): 23,\n ('make', 'a'): 22,\n ('any', 'of'): 22,\n ('the', 'other'): 22,\n ('of', 'The'): 22,\n ('out', 'to'): 22,\n ('what', 'it'): 22,\n ('that', 'are'): 22,\n ('just', 'as'): 22,\n ('What', \"'s\"): 22,\n ('and', 'I'): 22,\n (',', 'more'): 22,\n ('to', 'take'): 22,\n ('and', 'then'): 22,\n ('make', 'the'): 22,\n ('its', 'characters'): 22,\n ('you', 'might'): 22,\n ('the', 'filmmakers'): 22,\n ('of', 'that'): 22,\n ('story', 'of'): 22,\n ('special', 'effects'): 22,\n ('New', 'York'): 22,\n ('fact', 'that'): 22,\n ('care', 'about'): 22,\n ('feel', 'like'): 22,\n (\"'s\", 'so'): 22,\n ('...', 'and'): 22,\n ('the', 'last'): 22,\n ('what', \"'s\"): 22,\n ('not', 'only'): 22,\n ('that', 'this'): 22,\n (',', 'especially'): 22,\n ('can', 'not'): 21,\n ('part', 'of'): 21,\n ('examination', 'of'): 21,\n (\"'ve\", 'seen'): 21,\n ('a', 'long'): 21,\n ('story', 'that'): 21,\n ('is', 'about'): 21,\n ('soap', 'opera'): 21,\n (',', 'yet'): 21,\n ('but', 'also'): 21,\n (\"'s\", 'most'): 21,\n (',', '``'): 21,\n ('and', 'you'): 21,\n ('the', 'fact'): 21,\n (',', 'they'): 21,\n ('just', 'a'): 21,\n ('that', 'we'): 21,\n ('movies', ','): 21,\n ('makes', 'the'): 21,\n ('there', 'are'): 21,\n ('it', 'to'): 21,\n ('you', 'have'): 21,\n ('into', 'an'): 21,\n ('minutes', 'of'): 21,\n ('running', 'time'): 21,\n ('people', 'who'): 21,\n ('the', 'very'): 21,\n ('of', 'them'): 21,\n (',', '`'): 21,\n ('series', 'of'): 21,\n ('tries', 'to'): 21,\n (\"'s\", 'also'): 21,\n ('should', 'have'): 21,\n ('The', 'script'): 21,\n ('the', 'script'): 21,\n ('the', 'whole'): 20,\n ('like', 'this'): 20,\n ('make', 'it'): 20,\n ('makes', 'it'): 20,\n ('might', 'be'): 20,\n ('and', 'some'): 20,\n ('love', 'story'): 20,\n (\"n't\", 'be'): 20,\n ('performances', ','): 20,\n ('as', 'much'): 20,\n ('how', 'to'): 20,\n ('their', 'own'): 20,\n ('well', 'as'): 20,\n ('one', 'that'): 20,\n ('thriller', '.'): 20,\n ('cast', 'is'): 20,\n ('good', ','): 20,\n ('you', 'to'): 20,\n ('cast', ','): 20,\n ('the', 'more'): 20,\n ('performances', '.'): 20,\n ('out', 'the'): 20,\n (\"'s\", 'best'): 20,\n ('But', 'it'): 20,\n (\"'s\", 'all'): 20,\n ('could', 'be'): 20,\n ('when', 'the'): 20,\n ('much', 'more'): 20,\n ('times', ','): 20,\n ('with', 'his'): 20,\n ('and', 'as'): 20,\n ('films', '.'): 20,\n ('all', 'that'): 20,\n ('but', 'a'): 20,\n ('the', 'material'): 20,\n ('material', '.'): 20,\n (';', 'it'): 20,\n ('plays', 'like'): 20,\n ('``', 'The'): 19,\n ('but', 'I'): 19,\n ('up', 'for'): 19,\n ('heart', '.'): 19,\n ('good', 'time'): 19,\n ('at', 'its'): 19,\n ('movie', 'in'): 19,\n ('years', '.'): 19,\n ('make', 'you'): 19,\n ('the', 'performances'): 19,\n ('to', 'go'): 19,\n ('much', 'as'): 19,\n (',', 'too'): 19,\n ('up', 'the'): 19,\n ('the', 'picture'): 19,\n ('of', 'your'): 19,\n ('the', 'cast'): 19,\n ('in', 'their'): 19,\n ('humor', 'and'): 19,\n ('movies', '.'): 19,\n ('way', ','): 19,\n ('bit', 'of'): 19,\n ('with', 'such'): 19,\n ('are', \"n't\"): 19,\n ('looking', 'for'): 19,\n ('here', ','): 19,\n ('even', 'if'): 19,\n ('of', 'life'): 19,\n ('of', 'one'): 19,\n ('all', '.'): 19,\n (',', 'funny'): 19,\n ('work', ','): 19,\n ('does', 'not'): 19,\n ('feels', 'like'): 19,\n ('when', 'you'): 19,\n ('that', 'I'): 19,\n ('as', 'its'): 19,\n ('movie', 'about'): 19,\n ('its', 'title'): 19,\n ('more', 'like'): 19,\n ('of', '``'): 18,\n (',', \"''\"): 18,\n ('fun', ','): 18,\n (\"n't\", 'a'): 18,\n ('it', 'makes'): 18,\n ('the', 'point'): 18,\n ('for', 'an'): 18,\n ('one', 'is'): 18,\n ('experience', '.'): 18,\n ('and', 'most'): 18,\n ('amount', 'of'): 18,\n ('off', 'as'): 18,\n ('see', 'it'): 18,\n ('a', 'big'): 18,\n ('from', 'its'): 18,\n ('to', 'look'): 18,\n ('of', 'humor'): 18,\n (')', '.'): 18,\n ('year', \"'s\"): 18,\n ('an', 'interesting'): 18,\n ('without', 'the'): 18,\n ('it', 'were'): 18,\n ('level', 'of'): 18,\n ('(', 'A'): 18,\n ('A', ')'): 18,\n ('power', 'of'): 18,\n ('cinema', '.'): 18,\n ('again', '.'): 18,\n ('and', 'their'): 18,\n ('the', 'time'): 18,\n ('The', 'only'): 18,\n ('us', 'to'): 18,\n ('is', 'no'): 18,\n ('good', '.'): 18,\n ('that', 'has'): 18,\n ('While', 'the'): 18,\n ('point', 'of'): 18,\n ('seem', 'to'): 18,\n ('The', 'most'): 18,\n ('plenty', 'of'): 18,\n ('long', ','): 18,\n ('but', 'ultimately'): 18,\n ('need', 'to'): 18,\n ('are', 'so'): 18,\n ('the', 'subject'): 18,\n ('but', 'he'): 18,\n ('likely', 'to'): 18,\n ('the', 'title'): 18,\n ('the', 'only'): 18,\n ('supposed', 'to'): 18,\n ('is', 'still'): 17,\n ('acting', ','): 17,\n ('a', 'new'): 17,\n (')', 'and'): 17,\n (',', 'sometimes'): 17,\n ('a', 'fascinating'): 17,\n ('that', 'would'): 17,\n ('smart', ','): 17,\n ('it', 'will'): 17,\n ('than', 'its'): 17,\n ('films', 'that'): 17,\n ('humor', ','): 17,\n ('who', 'has'): 17,\n ('the', 'viewer'): 17,\n ('a', 'certain'): 17,\n ('and', 'in'): 17,\n ('a', 'young'): 17,\n ('of', 'movie'): 17,\n ('watch', '.'): 17,\n ('story', 'about'): 17,\n ('he', 'has'): 17,\n ('as', 'one'): 17,\n ('of', 'us'): 17,\n ('and', 'often'): 17,\n ('of', 'our'): 17,\n ('for', 'this'): 17,\n ('but', 'that'): 17,\n ('of', 'love'): 17,\n ('a', 'director'): 17,\n ('you', 'think'): 17,\n ('way', 'that'): 17,\n ('man', \"'s\"): 17,\n ('more', '.'): 17,\n ('is', 'like'): 17,\n ('entertainment', '.'): 17,\n ('or', 'a'): 17,\n ('it', 'also'): 17,\n ('itself', '.'): 17,\n ('the', 'actors'): 17,\n ('about', 'as'): 17,\n (\"n't\", '.'): 17,\n ('way', 'of'): 17,\n ('minutes', ','): 17,\n ('problem', 'with'): 17,\n ('up', 'with'): 17,\n ('short', 'of'): 17,\n ('an', 'hour'): 17,\n ('but', 'its'): 17,\n ('in', 'all'): 17,\n ('that', 'a'): 16,\n ('or', 'not'): 16,\n ('that', 'even'): 16,\n ('to', 'it'): 16,\n ('you', '.'): 16,\n ('take', 'on'): 16,\n ('very', 'funny'): 16,\n (\"'s\", 'performance'): 16,\n ('ability', 'to'): 16,\n ('of', 'her'): 16,\n ('and', 'yet'): 16,\n ('far', 'more'): 16,\n ('film', 'to'): 16,\n ('been', 'a'): 16,\n ('life', ','): 16,\n ('a', 'man'): 16,\n ('between', 'the'): 16,\n ('and', 'The'): 16,\n ('performance', '.'): 16,\n ('for', 'it'): 16,\n ('with', 'all'): 16,\n ('you', 'feel'): 16,\n ('character', ','): 16,\n ('a', 'comedy'): 16,\n ('have', 'made'): 16,\n ('or', 'even'): 16,\n ('close', 'to'): 16,\n ('is', 'funny'): 16,\n (',', 'of'): 16,\n ('and', 'is'): 16,\n (\"'s\", 'as'): 16,\n ('has', \"n't\"): 16,\n ('becomes', 'a'): 16,\n ('is', 'nothing'): 16,\n (',', '('): 16,\n ('will', 'probably'): 16,\n ('compelling', '.'): 16,\n ('big', 'screen'): 16,\n ('for', 'all'): 16,\n ('be', ','): 16,\n ('at', 'all'): 16,\n ('they', \"'re\"): 16,\n ('they', 'are'): 16,\n ('attempt', 'to'): 16,\n ('that', 'should'): 16,\n ('of', 'being'): 16,\n ('lots', 'of'): 16,\n ('enough', '.'): 16,\n ('a', 'real'): 16,\n (',', 'just'): 16,\n ('impossible', 'to'): 16,\n ('to', 'care'): 16,\n ('picture', '.'): 16,\n ('but', 'this'): 16,\n ('for', 'that'): 16,\n ('by', 'an'): 16,\n ('up', 'a'): 16,\n ('is', 'its'): 16,\n ('under', 'the'): 16,\n ('all', ','): 16,\n ('we', 'have'): 16,\n ('even', 'more'): 16,\n ('a', 'series'): 16,\n ('dialogue', 'and'): 16,\n ('minutes', '.'): 16,\n (\"n't\", 'quite'): 16,\n ('dull', ','): 16,\n ('never', 'quite'): 16,\n ('dialogue', ','): 15,\n ('be', 'as'): 15,\n ('out', '.'): 15,\n ('a', 'whole'): 15,\n ('more', 'of'): 15,\n (\"'s\", 'been'): 15,\n ('performances', 'by'): 15,\n ('come', 'to'): 15,\n ('--', 'as'): 15,\n ('those', 'who'): 15,\n (',', 'however'): 15,\n ('even', 'the'): 15,\n ('It', 'has'): 15,\n ('comes', 'to'): 15,\n ('as', 'they'): 15,\n ('will', 'have'): 15,\n ('and', 'just'): 15,\n ('the', 'action'): 15,\n ('the', 'heart'): 15,\n ('director', ','): 15,\n ('love', '.'): 15,\n ('will', 'find'): 15,\n ('could', \"n't\"): 15,\n ('may', 'have'): 15,\n ('reason', 'to'): 15,\n ('many', 'of'): 15,\n ('film', 'does'): 15,\n ('or', 'the'): 15,\n ('back', 'to'): 15,\n ('boring', '.'): 15,\n ('the', 'rest'): 15,\n ('is', 'all'): 15,\n ('filled', 'with'): 15,\n ('study', 'of'): 15,\n ('it', 'can'): 15,\n ('easy', 'to'): 15,\n ('director', \"'s\"): 15,\n ('I', 'ca'): 15,\n ('one', ','): 15,\n ('to', 'an'): 15,\n ('--', 'is'): 15,\n (',', 'despite'): 15,\n ('only', 'to'): 15,\n (',', 'some'): 15,\n ('It', 'may'): 15,\n ('to', 'his'): 15,\n (\"'ll\", 'be'): 15,\n ('where', 'the'): 15,\n (\"n't\", 'really'): 15,\n ('none', 'of'): 15,\n ('Like', 'a'): 15,\n ('anyone', 'who'): 15,\n ('people', '.'): 15,\n (',', 'by'): 15,\n ('little', 'more'): 15,\n ('instead', 'of'): 15,\n ('mess', '.'): 15,\n ('who', 'is'): 15,\n (\"'s\", 'nothing'): 15,\n ('has', 'no'): 15,\n ('that', 'made'): 15,\n ('movie', 'has'): 15,\n ('90', 'minutes'): 15,\n ('a', 'sense'): 15,\n ('drama', ','): 15,\n ('that', 'never'): 15,\n (';', 'the'): 15,\n ('bad', '.'): 15,\n (\"''\", 'and'): 14,\n ('captures', 'the'): 14,\n ('might', 'not'): 14,\n ('even', 'a'): 14,\n ('this', 'year'): 14,\n ('idea', 'of'): 14,\n ('that', 'takes'): 14,\n ('approach', 'to'): 14,\n ('De', 'Niro'): 14,\n ('with', 'this'): 14,\n ('movies', 'that'): 14,\n ('the', 'screenplay'): 14,\n ('in', 'years'): 14,\n ('performances', 'are'): 14,\n ('a', 'solid'): 14,\n ('too', 'long'): 14,\n ('thanks', 'to'): 14,\n ('makes', 'you'): 14,\n ('to', 'tell'): 14,\n (',', 'from'): 14,\n ('premise', '.'): 14,\n (\"'s\", 'still'): 14,\n ('but', 'in'): 14,\n ('has', 'its'): 14,\n ('world', '.'): 14,\n ('entertaining', '.'): 14,\n ('This', 'movie'): 14,\n ('down', 'the'): 14,\n ('and', 'no'): 14,\n ('script', '.'): 14,\n ('the', 'human'): 14,\n (\"n't\", 'know'): 14,\n ('ride', '.'): 14,\n (',', 'most'): 14,\n ('the', 'power'): 14,\n ('scenes', 'of'): 14,\n ('a', 'documentary'): 14,\n ('meditation', 'on'): 14,\n ('entertaining', ','): 14,\n ('to', 'show'): 14,\n (\"n't\", 'make'): 14,\n ('wit', 'and'): 14,\n (\"'s\", 'too'): 14,\n ('dull', '.'): 14,\n ('not', 'the'): 14,\n ('to', 'believe'): 14,\n ('ever', 'made'): 14,\n ('we', \"'re\"): 14,\n ('has', 'all'): 14,\n ('better', '.'): 14,\n ('much', 'to'): 14,\n ('it', 'may'): 14,\n ('As', 'a'): 14,\n ('I', \"'d\"): 14,\n (',', 'only'): 14,\n ('history', '.'): 14,\n ('--', 'that'): 14,\n ('in', 'your'): 14,\n ('a', 'strong'): 14,\n ('movie', 'does'): 14,\n ('and', 'funny'): 14,\n ('like', 'it'): 14,\n ('and', 'this'): 14,\n ('film', '...'): 14,\n ('that', '.'): 14,\n ('it', 'could'): 14,\n ('again', ','): 14,\n ('I', 'do'): 14,\n ('flick', '.'): 14,\n ('see', 'the'): 14,\n ('that', 'was'): 14,\n ('she', \"'s\"): 14,\n (\"'s\", 'something'): 14,\n ('--', 'it'): 14,\n ('film', 'in'): 14,\n ('moments', '.'): 14,\n ('a', 'better'): 14,\n ('plot', 'and'): 14,\n ('wants', 'to'): 14,\n ('made', 'a'): 14,\n ('ends', 'up'): 14,\n ('like', 'an'): 14,\n ('The', 'problem'): 14,\n (\"'s\", 'really'): 13,\n ('a', 'rather'): 13,\n ('to', 'work'): 13,\n ('world', 'of'): 13,\n ('and', 'ultimately'): 13,\n ('mix', 'of'): 13,\n ('you', ','): 13,\n ('beyond', 'the'): 13,\n ('comes', 'off'): 13,\n ('is', 'also'): 13,\n ('makes', 'a'): 13,\n ('movies', 'of'): 13,\n ('are', 'the'): 13,\n ('suffers', 'from'): 13,\n ('its', 'best'): 13,\n ('The', 'best'): 13,\n ('woman', \"'s\"): 13,\n ('other', 'than'): 13,\n ('the', 'people'): 13,\n ('genre', '.'): 13,\n ('history', ','): 13,\n ('Despite', 'its'): 13,\n ('worth', 'seeing'): 13,\n ('actors', ','): 13,\n ('ever', 'seen'): 13,\n ('not', 'as'): 13,\n ('that', 'can'): 13,\n ('Even', 'if'): 13,\n ('you', 'do'): 13,\n ('heart', ','): 13,\n (',', 'making'): 13,\n ('made', '.'): 13,\n ('love', 'and'): 13,\n ('a', 'powerful'): 13,\n ('but', 'what'): 13,\n ('love', ','): 13,\n ('has', 'some'): 13,\n ('far', 'from'): 13,\n ('a', 'way'): 13,\n ('a', 'single'): 13,\n ('is', 'never'): 13,\n ('only', 'a'): 13,\n ('about', 'it'): 13,\n ('a', 'couple'): 13,\n ('of', 'two'): 13,\n ('scenes', '.'): 13,\n ('was', 'a'): 13,\n ('out', ','): 13,\n ('but', 'they'): 13,\n ('leave', 'you'): 13,\n ('difficult', 'to'): 13,\n ('style', 'and'): 13,\n (',', 'are'): 13,\n ('But', 'the'): 13,\n ('in', '.'): 13,\n ('time', 'to'): 13,\n ('takes', 'a'): 13,\n ('him', '.'): 13,\n ('...', 'it'): 13,\n ('who', \"'s\"): 13,\n (',', 'what'): 13,\n ('was', \"n't\"): 13,\n ('end', 'of'): 13,\n ('For', 'all'): 13,\n (',', 'has'): 13,\n ('less', 'than'): 13,\n ('the', 'final'): 13,\n ('You', 'can'): 13,\n ('the', 'real'): 13,\n ('down', 'to'): 13,\n ('very', 'good'): 13,\n ('has', 'made'): 13,\n ('devoid', 'of'): 13,\n ('that', 'there'): 13,\n ('premise', ','): 13,\n ('script', 'is'): 13,\n ('You', \"'ll\"): 12,\n ('what', 'makes'): 12,\n ('worth', 'the'): 12,\n ('most', 'part'): 12,\n ('exploration', 'of'): 12,\n ('beautiful', ','): 12,\n ('and', 'to'): 12,\n ('away', 'from'): 12,\n ('see', 'this'): 12,\n ('tale', '.'): 12,\n ('at', 'once'): 12,\n ('because', 'of'): 12,\n ('characters', \"'\"): 12,\n ('being', 'a'): 12,\n ('say', ','): 12,\n ('the', 'new'): 12,\n ('of', 'work'): 12,\n ('children', \"'s\"): 12,\n ('performances', 'and'): 12,\n (',', 'really'): 12,\n ('be', 'an'): 12,\n ('...', 'an'): 12,\n ('these', 'days'): 12,\n ('result', 'is'): 12,\n ('world', \"'s\"): 12,\n ('rest', 'of'): 12,\n ('up', 'on'): 12,\n ('fans', 'of'): 12,\n ('this', '.'): 12,\n (\"'\", '.'): 12,\n ('once', 'again'): 12,\n ('and', 'very'): 12,\n ('than', 'to'): 12,\n ('I', 'was'): 12,\n ('the', '`'): 12,\n ('that', 'have'): 12,\n ('not', 'very'): 12,\n ('a', 'while'): 12,\n ('drama', 'about'): 12,\n ('away', '.'): 12,\n ('and', 'director'): 12,\n ('of', 'how'): 12,\n ('see', 'a'): 12,\n ('think', 'of'): 12,\n ('The', 'plot'): 12,\n ('I', \"'ll\"): 12,\n (\"'\", 'is'): 12,\n ('charm', '.'): 12,\n ('cast', '.'): 12,\n ('we', \"'ve\"): 12,\n ('dark', ','): 12,\n ('style', '.'): 12,\n ('`', 'The'): 12,\n (\"'re\", 'not'): 12,\n ('(', 'the'): 12,\n ('lacks', 'the'): 12,\n ('be', 'more'): 12,\n ('suspense', '.'): 12,\n ('things', 'that'): 12,\n ('of', 'good'): 12,\n ('of', 'Fire'): 12,\n ('your', 'heart'): 12,\n ('as', 'any'): 12,\n ('the', '``'): 12,\n ('film', 'never'): 12,\n ('Blue', 'Crush'): 12,\n ('that', 'could'): 12,\n ('episode', 'of'): 12,\n ('to', 'sit'): 12,\n ('not', 'enough'): 12,\n ('to', 'create'): 12,\n ('script', ','): 12,\n ('all', 'too'): 12,\n ('Do', \"n't\"): 12,\n ('sit', 'through'): 12,\n ('are', 'too'): 12,\n ('bad', ','): 12,\n ('had', 'a'): 12,\n (\"n't\", 'even'): 12,\n ('character', 'study'): 11,\n ('year', ','): 11,\n (',', 'often'): 11,\n ('the', 'one'): 11,\n ('it', 'comes'): 11,\n (',', 'his'): 11,\n ('?', \"''\"): 11,\n ('predictable', ','): 11,\n ('time', 'and'): 11,\n ('turns', 'out'): 11,\n ...}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=train[:,0]\n",
    "labels=train[:,1]\n",
    "bag_of_words={}\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    for word1,word2 in zip(words,words[1:]):\n",
    "        bigram=(word1,word2)\n",
    "        bag_of_words[bigram]=bag_of_words.get(bigram,0)+1\n",
    "bag_of_words=dict(sorted(bag_of_words.items(),key=lambda kv:-kv[1] ))\n",
    "bag_of_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:15:26.244843800Z",
     "start_time": "2024-03-15T23:15:26.070650800Z"
    }
   },
   "id": "19fa29ac0c55be09"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8544, 87249)\n"
     ]
    }
   ],
   "source": [
    "x_train_sentence_vectors = np.zeros((len(sentences), len(bag_of_words)), dtype=int)\n",
    "bi_gram_index = {bi_gram: index for index, bi_gram in enumerate(bag_of_words)}\n",
    "for i, sentence in enumerate(sentences):\n",
    "    words = sentence.split()\n",
    "    bigrams=set()\n",
    "    for word1,word2 in zip(words,words[1:]):\n",
    "        bigrams.add((word1,word2))\n",
    "    for bi_gram in bigrams:\n",
    "        if bi_gram in bi_gram_index:\n",
    "            index = bi_gram_index[bi_gram]\n",
    "            x_train_sentence_vectors[i, index] = 1\n",
    "print(x_train_sentence_vectors.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:15:27.415201500Z",
     "start_time": "2024-03-15T23:15:27.145676600Z"
    }
   },
   "id": "cf46f9aab3497ed3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "del x_train_sentence_vectors\n",
    "del bi_gram_index\n",
    "del bag_of_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:15:28.302675700Z",
     "start_time": "2024-03-15T23:15:28.239566800Z"
    }
   },
   "id": "5d5194a8d4987fc5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6944400072097778 0.833329975605011 0.625 ... 0.75 0.13888999819755554\n",
      " 0.3472200036048889]\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "def maping(a):\n",
    "    if a<=0.2:\n",
    "        return 0       #'very negative'\n",
    "    if a<=0.4 and a>0.2:\n",
    "        return 1       # \"negative\"\n",
    "    if a<=0.6 and a>0.4:\n",
    "        return 2       # \"neutral\"\n",
    "    if a<=0.8 and a>0.6:\n",
    "        return 3       # \"positive\"\n",
    "    if a<=1 and a>0.8:\n",
    "        return 4       # \"very positive\"\n",
    "y_train= np.vectorize(maping)(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:15:29.024174500Z",
     "start_time": "2024-03-15T23:15:29.006778700Z"
    }
   },
   "id": "76a851d5c11461e5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class logistic_regretion:\n",
    "    def __init__(self):\n",
    "        self.bias = None\n",
    "        self.weights = None\n",
    "        self._bag_of_words=None\n",
    "        self._sentence_vector=None\n",
    "        self.accuracy=None\n",
    "        self._labels=None\n",
    "    def one_hot_encode(self, y, num_classes):\n",
    "        one_hot_y = np.zeros((len(y), num_classes))\n",
    "        one_hot_y[np.arange(len(y)), y] = 1\n",
    "        return one_hot_y\n",
    "    def softmax(self,z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    def _x_vectorized(self,sentences):\n",
    "        x_train_sentence_vectors = np.zeros((len(sentences), len(self._bag_of_words)), dtype=int)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            words = sentence.split()\n",
    "            bigrams=set()\n",
    "            for word1,word2 in zip(words,words[1:]):\n",
    "                 bigrams.add((word1,word2))\n",
    "            for bi_gram in bigrams:\n",
    "                if bi_gram in self._bag_of_words:\n",
    "                    index = self._bag_of_words[bi_gram]\n",
    "                    x_train_sentence_vectors[i, index] = 1\n",
    "        return x_train_sentence_vectors\n",
    "        \n",
    "    def fit(self,sentences,labels):\n",
    "        bag_of_words={}\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            for word1,word2 in zip(words,words[1:]):\n",
    "                bigram=(word1,word2)\n",
    "                bag_of_words[bigram]=bag_of_words.get(bigram,0)+1\n",
    "        bag_of_words=dict(sorted(bag_of_words.items(),key=lambda kv:-kv[1] ))\n",
    "        self._bag_of_words= {bi_gram: index for index, bi_gram in enumerate(bag_of_words)}\n",
    "        self._sentence_vector=self._x_vectorized(sentences)\n",
    "        self._labels=labels\n",
    "    def compile(self,n_epochs,n_batches,learning_rate=0.001,l2_lambda=0.01):\n",
    "        print(\"starting training model...This could take a while\")\n",
    "        num_classes = len(np.unique(self._labels))\n",
    "        self.weights = np.zeros((self._sentence_vector.shape[1],num_classes))\n",
    "        y_one_hot = self.one_hot_encode(self._labels, num_classes)\n",
    "        self.bias = np.zeros((1, num_classes))\n",
    "        for epoch in range(n_epochs):\n",
    "            indices = np.random.permutation(self._sentence_vector.shape[0])\n",
    "            X_shuffled = self._sentence_vector[indices]\n",
    "            y_shuffled = y_one_hot[indices]\n",
    "            for i in range(0, self._sentence_vector.shape[0], n_batches):\n",
    "                X_batch = X_shuffled[i:i + n_batches]\n",
    "                y_batch = y_shuffled[i:i + n_batches]\n",
    "                for j in range(len(X_batch)):\n",
    "                    x_sample = X_batch[j].reshape(1, -1)\n",
    "                    y_sample = y_batch[j].reshape(1, -1)\n",
    "                    z=np.dot(x_sample,self.weights)+self.bias\n",
    "                    y_pred = self.softmax(z)    \n",
    "                    dz = y_pred - y_sample\n",
    "                    dw = np.dot(x_sample.T, dz) + 2 * l2_lambda * self.weights \n",
    "                    db = dz\n",
    "                    self.weights -= learning_rate * dw\n",
    "                    self.bias -= learning_rate * db.flatten()\n",
    "                print(f\"Epoch {epoch + 1}/{n_epochs},Batch {i // n_batches + 1}/{self._sentence_vector.shape[0] // n_batches},Accuracy : {self._accuracy(X_batch,np.argmax(y_batch, axis=1))}\")\n",
    "        self.accuracy=self._accuracy(self._sentence_vector,np.argmax(y_one_hot, axis=1))\n",
    "    def _predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.softmax(z)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "    def _accuracy(self, x, y):\n",
    "        predictions = self._predict(x)\n",
    "        # print(predictions.shape,\" dah shape el pred : \",y.shape)\n",
    "        correct_predictions = np.sum(predictions == y)\n",
    "        accuracy = correct_predictions / len(y)\n",
    "        return accuracy\n",
    "    def predict(self,sentences):\n",
    "        x_vector=self._x_vectorized(sentences)\n",
    "        return self._predict(x_vector)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:15:30.341083200Z",
     "start_time": "2024-03-15T23:15:30.321997100Z"
    }
   },
   "id": "b3bd9d73b3176f27"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training model...This could take a while\n",
      "Epoch 1/5,Batch 1/8,Accuracy : 0.4344569288389513\n",
      "Epoch 1/5,Batch 2/8,Accuracy : 0.4091760299625468\n",
      "Epoch 1/5,Batch 3/8,Accuracy : 0.4803370786516854\n",
      "Epoch 1/5,Batch 4/8,Accuracy : 0.4934456928838951\n",
      "Epoch 1/5,Batch 5/8,Accuracy : 0.2902621722846442\n",
      "Epoch 1/5,Batch 6/8,Accuracy : 0.2902621722846442\n",
      "Epoch 1/5,Batch 7/8,Accuracy : 0.299625468164794\n",
      "Epoch 1/5,Batch 8/8,Accuracy : 0.299625468164794\n",
      "Epoch 2/5,Batch 1/8,Accuracy : 0.38857677902621723\n",
      "Epoch 2/5,Batch 2/8,Accuracy : 0.37359550561797755\n",
      "Epoch 2/5,Batch 3/8,Accuracy : 0.3970037453183521\n",
      "Epoch 2/5,Batch 4/8,Accuracy : 0.4241573033707865\n",
      "Epoch 2/5,Batch 5/8,Accuracy : 0.4747191011235955\n",
      "Epoch 2/5,Batch 6/8,Accuracy : 0.39419475655430714\n",
      "Epoch 2/5,Batch 7/8,Accuracy : 0.37359550561797755\n",
      "Epoch 2/5,Batch 8/8,Accuracy : 0.3464419475655431\n",
      "Epoch 3/5,Batch 1/8,Accuracy : 0.38857677902621723\n",
      "Epoch 3/5,Batch 2/8,Accuracy : 0.38670411985018727\n",
      "Epoch 3/5,Batch 3/8,Accuracy : 0.38108614232209737\n",
      "Epoch 3/5,Batch 4/8,Accuracy : 0.3848314606741573\n",
      "Epoch 3/5,Batch 5/8,Accuracy : 0.4250936329588015\n",
      "Epoch 3/5,Batch 6/8,Accuracy : 0.4653558052434457\n",
      "Epoch 3/5,Batch 7/8,Accuracy : 0.4597378277153558\n",
      "Epoch 3/5,Batch 8/8,Accuracy : 0.42602996254681647\n",
      "Epoch 4/5,Batch 1/8,Accuracy : 0.47284644194756553\n",
      "Epoch 4/5,Batch 2/8,Accuracy : 0.4363295880149813\n",
      "Epoch 4/5,Batch 3/8,Accuracy : 0.45692883895131087\n",
      "Epoch 4/5,Batch 4/8,Accuracy : 0.47659176029962547\n",
      "Epoch 4/5,Batch 5/8,Accuracy : 0.44569288389513106\n",
      "Epoch 4/5,Batch 6/8,Accuracy : 0.42696629213483145\n",
      "Epoch 4/5,Batch 7/8,Accuracy : 0.46348314606741575\n",
      "Epoch 4/5,Batch 8/8,Accuracy : 0.44569288389513106\n",
      "Epoch 5/5,Batch 1/8,Accuracy : 0.41292134831460675\n",
      "Epoch 5/5,Batch 2/8,Accuracy : 0.42602996254681647\n",
      "Epoch 5/5,Batch 3/8,Accuracy : 0.46254681647940077\n",
      "Epoch 5/5,Batch 4/8,Accuracy : 0.4447565543071161\n",
      "Epoch 5/5,Batch 5/8,Accuracy : 0.45131086142322097\n",
      "Epoch 5/5,Batch 6/8,Accuracy : 0.43258426966292135\n",
      "Epoch 5/5,Batch 7/8,Accuracy : 0.47191011235955055\n",
      "Epoch 5/5,Batch 8/8,Accuracy : 0.46254681647940077\n",
      "0.45786516853932585\n"
     ]
    }
   ],
   "source": [
    "model=logistic_regretion()\n",
    "model.fit(sentences,y_train)\n",
    "model.compile(n_epochs=5,n_batches=sentences.shape[0]//8)\n",
    "print(model.accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:24:46.203426Z",
     "start_time": "2024-03-15T23:17:41.188510900Z"
    }
   },
   "id": "61184eb9c784ad1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]   [1503  707]\n",
      "Accuracy:  [31.99095023]\n"
     ]
    }
   ],
   "source": [
    "sentences=test[:,0]\n",
    "labels=test[:,1]\n",
    "y_test_true=np.vectorize(maping)(labels)\n",
    "y_test_prediction=model.predict(sentences)\n",
    "acc=y_test_prediction==y_test_true\n",
    "x,c=np.unique(acc,return_counts=True)\n",
    "print(x,\" \",c)\n",
    "print(\"Accuracy: \",(c[1]/y_test_prediction.shape)*100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:24:51.588294800Z",
     "start_time": "2024-03-15T23:24:50.127969100Z"
    }
   },
   "id": "2083cb5d60ea0c3c"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Classifier Accuracy: 0.4108597285067873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "vectorizer = TfidfVectorizer() \n",
    "X_train_vectorized = vectorizer.fit_transform(train[:,0])\n",
    "X_test_vectorized = vectorizer.transform(test[:,0])\n",
    "\n",
    "sgd_classifier = SGDClassifier( max_iter=50, alpha=0.001,random_state=42)  # Using logistic regression loss for SGD\n",
    "\n",
    "\n",
    "sgd_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "\n",
    "sgd_pred = sgd_classifier.predict(X_test_vectorized)\n",
    "\n",
    "print(\"SGD Classifier Accuracy:\", accuracy_score(y_test_true, sgd_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:25:07.793487700Z",
     "start_time": "2024-03-15T23:24:56.013790100Z"
    }
   },
   "id": "d7887bae977350eb"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.4099547511312217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zeyad\\Desktop\\computer communication\\compu 3rd year\\second term\\natural language processing\\assignment1\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(max_iter=50, random_state=42)\n",
    "logistic_regression.fit(X_train_vectorized, y_train)\n",
    "logistic_pred = logistic_regression.predict(X_test_vectorized)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test_true, logistic_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:25:12.229666300Z",
     "start_time": "2024-03-15T23:25:11.558330700Z"
    }
   },
   "id": "d7b003d1ae5ad934"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 41, 172,  22,  40,   4],\n       [ 28, 371,  87, 138,   9],\n       [  8, 166,  53, 149,  13],\n       [  4,  97,  47, 320,  42],\n       [  6,  31,  15, 226, 121]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusion_matrix_impl(true_labels, predicted_labels, labels=None):\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "    num_labels = len(labels)\n",
    "    cm = np.zeros((num_labels, num_labels), dtype=int)\n",
    "    for i in range(len(true_labels)):\n",
    "        true_idx = np.where(labels == true_labels[i])[0][0]\n",
    "        pred_idx = np.where(labels == predicted_labels[i])[0][0]\n",
    "        cm[true_idx, pred_idx] += 1\n",
    "    return cm\n",
    "confusionmatrix=confusion_matrix_impl(y_test_true,logistic_pred)\n",
    "confusionmatrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:35:04.164222100Z",
     "start_time": "2024-03-15T23:35:04.105547300Z"
    }
   },
   "id": "70cc0911538bda80"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision per class: [0.47126437 0.4432497  0.23660714 0.36655212 0.64021164]\n",
      "Recall per class: [0.14695341 0.58609795 0.13624679 0.62745098 0.30325815]\n",
      "F1 Score per class: [0.22404372 0.5047619  0.17292007 0.46276211 0.41156463]\n",
      "Macro-averaged Precision: 0.4315769942657062\n",
      "Macro-averaged Recall: 0.36000145273867934\n",
      "Macro-averaged F1 Score: 0.35521048461284543\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precision_score_per_class(confusion_matrix):\n",
    "    true_positives = np.diag(confusion_matrix)\n",
    "    false_positives = np.sum(confusion_matrix, axis=0) - true_positives\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    return precision\n",
    "\n",
    "def recall_score_per_class(confusion_matrix):\n",
    "    true_positives = np.diag(confusion_matrix)\n",
    "    false_negatives = np.sum(confusion_matrix, axis=1) - true_positives\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    return recall\n",
    "\n",
    "def f1_score_per_class(confusion_matrix):\n",
    "    precision = precision_score_per_class(confusion_matrix)\n",
    "    recall = recall_score_per_class(confusion_matrix)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "def macro_averaged_precision(confusion_matrix):\n",
    "    precision = precision_score_per_class(confusion_matrix)\n",
    "    return np.mean(precision)\n",
    "\n",
    "def macro_averaged_recall(confusion_matrix):\n",
    "    recall = recall_score_per_class(confusion_matrix)\n",
    "    return np.mean(recall)\n",
    "\n",
    "def macro_averaged_f1_score(confusion_matrix):\n",
    "    f1 = f1_score_per_class(confusion_matrix)\n",
    "    return np.mean(f1)\n",
    "\n",
    "\n",
    "print(\"Precision per class:\", precision_score_per_class(confusion_matrix=confusionmatrix))\n",
    "print(\"Recall per class:\", recall_score_per_class(confusion_matrix=confusionmatrix))\n",
    "print(\"F1 Score per class:\", f1_score_per_class(confusion_matrix=confusionmatrix))\n",
    "\n",
    "print(\"Macro-averaged Precision:\", macro_averaged_precision(confusion_matrix=confusionmatrix))\n",
    "print(\"Macro-averaged Recall:\", macro_averaged_recall(confusion_matrix=confusionmatrix))\n",
    "print(\"Macro-averaged F1 Score:\", macro_averaged_f1_score(confusion_matrix=confusionmatrix))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:30:41.496775900Z",
     "start_time": "2024-03-15T23:30:41.460243100Z"
    }
   },
   "id": "83910e7cd73f1277"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 41, 172,  22,  40,   4],\n       [ 28, 371,  87, 138,   9],\n       [  8, 166,  53, 149,  13],\n       [  4,  97,  47, 320,  42],\n       [  6,  31,  15, 226, 121]], dtype=int64)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "confusion_matrix(y_test_true,logistic_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:36:39.931527900Z",
     "start_time": "2024-03-15T23:36:39.908845600Z"
    }
   },
   "id": "69e640bdddbdecc7"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision per class: [0.47126437 0.4432497  0.23660714 0.36655212 0.64021164]\n",
      "Recall per class: [0.14695341 0.58609795 0.13624679 0.62745098 0.30325815]\n",
      "F1 Score per class: [0.22404372 0.5047619  0.17292007 0.46276211 0.41156463]\n",
      "Macro-averaged Precision: 0.4315769942657062\n",
      "Macro-averaged Recall: 0.36000145273867934\n",
      "Macro-averaged F1 Score: 0.35521048461284543\n"
     ]
    }
   ],
   "source": [
    "precision_per_class_skitl = precision_score(y_test_true,logistic_pred, average=None)\n",
    "\n",
    "recall_per_class_skitl = recall_score(y_test_true,logistic_pred, average=None)\n",
    "\n",
    "f1_score_per_class_skitl = f1_score(y_test_true,logistic_pred, average=None)\n",
    "\n",
    "macro_avg_precision_skitl = precision_score(y_test_true,logistic_pred, average='macro')\n",
    "\n",
    "macro_avg_recall_skitl = recall_score(y_test_true,logistic_pred, average='macro')\n",
    "\n",
    "macro_avg_f1_score_skitl = f1_score(y_test_true,logistic_pred, average='macro')\n",
    "\n",
    "print(\"Precision per class:\", precision_per_class_skitl)\n",
    "print(\"Recall per class:\",recall_per_class_skitl)\n",
    "print(\"F1 Score per class:\", f1_score_per_class_skitl)\n",
    "\n",
    "print(\"Macro-averaged Precision:\", macro_avg_precision_skitl)\n",
    "print(\"Macro-averaged Recall:\", macro_avg_recall_skitl)\n",
    "print(\"Macro-averaged F1 Score:\", macro_avg_f1_score_skitl)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T23:38:55.924047500Z",
     "start_time": "2024-03-15T23:38:55.822469Z"
    }
   },
   "id": "a83d20e691faab29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eb1e5e1d608d6bf2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
